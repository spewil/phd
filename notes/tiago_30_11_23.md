tiago_30_11_23.md

## questions for tal
- can we do policy gradient with MoG policy?
- how do we constrain the policy/exploration with e.g. KL to an existing policy?
- can we connect this regularized problem to inference? to information bottleneck? to linear/KL control theory? what is constrained/regularized/default policy optimization?
- can we connect the problem the subjects are facing? what kind of "inference" are people doing?
    - high-dimensional input from some generative distribution we're modeling with a GMM (data from the arm)
    - passing through a subspace/decoder (encoder?), which happens to be ~N(0,S)
    - people are looking at a projection (trajectories)
    - inferring what? they're going from latent space (e.g. mixture of gaussians, movements), to emg space (muscles, signals), to low-dim feedback space (trajectories, 2D). they're inferring what generated the trajectory? can we pose this in a nice way? are they inferring the decoder weights (thus can we try to infer them based on data)?

## tal notes
- X double check transposes and reshapes
- check that the nans in the data are doing the right thing
- check different levels of filtering impact on histograms
- X calibration data plot
- prior --> predicts performance per target
    - "lookup" the probability for a circle around the target, normalized by the histogram
    - divide the 2d space into target quadrants (of 12)
    - is this different from targets being equal?


## notes
- the EMG is already scaled according to each subject's variance from the calbrating, it should be fairly "whitened" and thus comparable across subjects
    - we can confirm this by recreating the "variance.bin" file (also looking at where this was done in the experimental notebooks)
- mean shifting still needs to be done-- and this needs to be done using a truly quiescent period, which could be tricky. we should probably use the "low point" as defined in the "activity filter"-- 

## reflections
- always just get a look at your data first
- what does the data look like? what is generating the data?
- this is your prior! this is what you think you know about the data source
- get an intuition for your data, it's features
- then think about generative models of the data
- LQR -- this might have been a good model in much later trials-- we're looking at people scrambling to learn the task 

## todo
- X natural movement -- subject, movement, session, channel, time
- X plot natural movement histograms, mixed movements, active only
- ? look at correlation between hit fractions and natural movement stats
    - " " " calibration stats
    - " " " trial stats (sanity check)
    - " " " gaussian stats (baseline)

- fit GMMs to emg data
    - deal with nans
    - sample the GMM, pass through decoder, compare to trajectory data (sanity check)
    - what do I do with these fits?
        - covariance structure--> covariance confined to subspace?

- redo the task and null space using latest data preprocessing

## story 
- methods, how the tasks work, data preprocessing
- performance, decoder, discussion about data manifold
- null and task space variability --> are subjects learning anything about the decoder?
- trajectory space densities--> performance correlations between prior and trial? entropy correlation?
- emg spatial variability-- discuss PCA vs. GMM --> low-rank gaussians explain trajectory distribution shape?
- policy gradient, discussion of natural stats bias? what inference problem are subjects solving?
- outlook -- how would we change our task to be a better comparison to algorithms?

### housekeeping

- backup notebooks? make some copies
- get the data into a reasonable shape,
    - mean offsets, be very clear about what you're doing with the data
    - generate "bricks" of data that are faster to work with
    - bricks for: natural movement, calibration, trials
    - be careful about mean-shifting, try to remove baselines in the data on ingest
- plot distributions in trajectory space for natural, calibration, and trials

### trajectories
- X take the mean of the histogram! instead of the mean of the interpolated...?
    - this doesn't really work, because you get over-representation of the "ends" of longer trajectories

### GMM
- write down what the mixture model means as a generator of our data, p(x) ~ ???
- fit GMMs to ??? and look at these fits -- do they look like our data? AIC/BIC? 
- inspect GMM fits and compare them across subjects, wrt constraints--
    - are these predictive of performance? means, number of clusters, shape?
    - how do the fits compare to each other? 
    - do we start to see interpretable differences between subjects?
- run GMMs through the decoder and plot data in trajectory space-- overlay activity

### PCA
- we should be able to do cross-target, cross-movement PCA, if we take into account "all" of the data distribution, such that centering is ok-- is this also true for fitting GMMs?
- what's the question here? PCA is essentially fitting a low-rank Gaussian to the data -- p(x) ~ N(0, WW.T + eye(N)*sigma^2) where W ε NxK, x ε N, K is the latent dimension. p(x|z) ~ N(Wz,eye(N)*sigma^2), z ~ N(0,eye(K)). think about what the latent variables would be in this case, K-dim samples from a spherical gaussian, then rotated and stretched into the N-dim data space, with K < N.
- if all the 

### visualizations
- can i make some GMM plots in 2D, do everything in 2D, to compare NMF (weights), PCA (PCs), GMMs (means), etc?
- understand REINFORCE
- understand how to numerically solve a regularized version of REINFORCE