tiago_30_11_23.md

## questions for tal
- can we do policy gradient with MoG policy?
- how do we constrain the policy/exploration with e.g. KL to an existing policy?
- can we connect this regularized problem to inference? to information bottleneck? to linear/KL control theory? what is constrained/regularized/default policy optimization?
- can we connect the problem the subjects are facing? what kind of "inference" are people doing?
    - high-dimensional input from some generative distribution we're modeling with a GMM (data from the arm)
    - passing through a subspace/decoder (encoder?), which happens to be ~N(0,S)
    - people are looking at a projection (trajectories)
    - inferring what? they're going from latent space (e.g. mixture of gaussians, movements), to emg space (muscles, signals), to low-dim feedback space (trajectories, 2D). they're inferring what generated the trajectory? can we pose this in a nice way? are they inferring the decoder weights (thus can we try to infer them based on data)?

## notes
- the EMG is already scaled according to each subject's variance from the calbrating, it should be fairly "whitened" and thus comparable across subjects
    - we can confirm this by recreating the "variance.bin" file (also looking at where this was done in the experimental notebooks)
- mean shifting still needs to be done-- and this needs to be done using a truly quiescent period, which could be tricky. we should probably use the "low point" as defined in the "activity filter"-- 

## reflections
- always just get a look at your data first
- what does the data look like? what is generating the data?
- this is your prior! this is what you think you know about the data source
- get an intuition for your data, it's features
- then think about generative models of the data

## todo

### housekeeping

- git clone repo again, in a different folder
- do the BFG thing on that repo
- move files manually into that new repo
- keep notebooks in dropbox?
- backup with what?

- get the data into a reasonable shape,
    - mean offsets, be very clear about what you're doing with the data
    - generate "bricks" of data that are faster to work with
    - bricks for: natural movement, calibration, trials
    - be careful about mean-shifting, try to remove baselines in the data on ingest
- plot distributions in trajectory space for natural, calibration, and trials

### trajectories
- take the mean of the histogram! instead of the mean of the interpolated...?

### GMM
- write down what the mixture model means as a generator of our data, p(x) ~ ???
- fit GMMs to ??? and look at these fits -- do they look like our data? AIC/BIC? 
- inspect GMM fits and compare them across subjects, wrt constraints--
    - are these predictive of performance? means, number of clusters, shape?
    - how do the fits compare to each other? 
    - do we start to see interpretable differences between subjects?
- run GMMs through the decoder and plot data in trajectory space-- overlay activity

### PCA
- we should be able to do cross-target, cross-movement PCA, if we take into account "all" of the data distribution, such that centering is ok-- is this also true for fitting GMMs?
- what's the question here? PCA is essentially fitting a low-rank Gaussian to the data -- p(x) ~ N(0, WW.T + eye(N)*sigma^2) where W ε NxK, x ε N, K is the latent dimension. p(x|z) ~ N(Wz,eye(N)*sigma^2), z ~ N(0,eye(K)). think about what the latent variables would be in this case, K-dim samples from a spherical gaussian, then rotated and stretched into the N-dim data space, with K < N.
- if all the 

### visualizations
- can i make some GMM plots in 2D, do everything in 2D, to compare NMF (weights), PCA (PCs), GMMs (means), etc?

- understand REINFORCE
- understand how to numerically solve a regularized version of REINFORCE
- 