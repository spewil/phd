\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../../images/}}}
\begin{document}

\chapter{Experimental Background}\label{chap:bg_experimental}

\begin{itemize}
\tightlist
\item
  what are the most important concepts / results that inform our
  experiments?
\item
  What do we think we know from experiments about motor control? motor
  adaptation? motor learning?
\end{itemize}

# Preliminary Experiments

{#sec:experiment}

> *We have some idea as to the intricate design of the puppet and the puppet strings, but we lack insight into the mind of the puppeteer.*
>
> &mdash; Bizzi & Ajemian, *2020*

\section{Related Experiments and Inspiration}

\section{Goals}

- measure across learning
- analyze this learning sequence to find traces of what's being learned at a trajectory level
- gain inspiration for what might be modeled

## Recording Setup

<!-- I would be most interested to hear how u are thinking of approaching the analysis. I.e. u have a bunch of channels, movements, tasks what is the workflow to get from that raw data into something manageable/useful. -->


<!-- The long term goal of the research direction suggested here is to develop tasks which ask subjects to produce a variety of movements in response to a variety of goals and perturbations. This will allow us to study the computations that humans use in everyday tasks to solve the motor problems they face. This stands in contrast to many repetitions of the same movements. However, we wish to validate our experimental setup on classical tasks as a stepping stone to tasks with greater variety. -->

<!-- "we know how to design and interpret experiments that involve many repetitions of the same movement however there is limited role for online optimization in that context. instead we need experiments where subjects are required to come up with new movements all the time. how can we get experimenters to do such experiments? show cool movies of robots doing cool things,and hopefully get the experimenters excited." (todorov online optimization slides) -->

The concept of the experimental setup is shown in {+@fig:setup}, where 32 monopolar electrodes are attached to a subject’s forearm to record muscle activity. The arm and hand are kinematically constrained in a custom fixture and motor activity is recorded during low-level isometric muscle contractions. The setup circumvents the limb biomechanics by mapping muscle output directly to virtual stimuli shown on a screen. By focusing on low-force, isometric contractions we intend to avoid complications due to artifacts in dynamic, high-force movements.

![(a) Graphic depicting the closed-loop EMG interface concept in a center-hold, reach-out type task. The multidimensional EMG signal is transformed online through a mapping $F$ from EMG electrode space to a lower dimensional task space. In experiments shown here the task space is a two-dimensional, though the EMG interface can be extended to tasks with higher-dimensional inputs. The subject's arm and hand are constrained during the experiment to ensure isometric contractions. (b) First prototype of custom recording hardware consisting of four bands of eight electrodes each, and a spherical hand constraint. Our recordings are 32 channel monopolar recording with reference electrode at the wrist. (c) Example cup-style monopolar recording electrodes, 5mm in diameter. (d) Side view of the recording hardware. Also pictured is the arm restraint frame to ensure isometric contractions. The frame obscures the subject's arm from view and contains adjustable elbow and wrist rests. (d) Recording hardware shown off the arm with wireless amplifier and connection board.](images/hardware/setup.pdf){width=100% #fig:setup}

As far as we are aware, this setup is novel in combining a high number of channels with an abstract mapping. Learning experiments have used joint angles and a few muscles (typically movements of the wrist or pairs of thumb and intrinsic hand muscles), but none have taken a data-driven approach in constructing a virtual learning environment in the style of cortical BMI[@BergerDifferencesInAdaptationRates2013a;@Dyson2018;@radhakrishnanLearningNovelMyoelectricControlled2008;@Gallego2017]. Our EMG recording setup is custom-built: the "Sessantaquattro" EMG amplifier was acquired from OT Bioelettronica, the electronic connector was designed in-house, the electrodes were acquired from Medkit UK, and the recording software was written in a mixture of Bonsai (C#) and Python. EMG is acquired at 2kHz sample rate with 24-bit precision. A clip of raw data is shown in {+@fig:raw_data}.

![10 seconds of raw 32-channel EMG data taken during a minimal finger flexion trial. Note that some channels include a nontrivial amount of line noise. This noise will be drastically reduced through changes being made in the next recording hardware revision which include shielding, shorter cables, and better cable routing. Note that some channels (e.g. channel 24) show very low noise and putative single motor unit action potentials can be seen on many channels.](images/data_analysis/fingers/raw_data.pdf){width=100% #fig:raw_data}

To preprocessing the data, simple filtering and rectifying were applied, as is commonly done in the literature[@sangerBayesianFilteringMyoelectric2007;@churchlandNeuralPopulationDynamics2012a;@churchlandNeuralVariabilityPremotor2006;@sussillo2015]. As shown in {+@fig:preprocessing_steps}, here we apply highpass filtering at 40Hz to remove any low-frequency oscillations and DC offsets, rectification and lowpass filtering at 5Hz to extract what is typically associated with a force readout of the EMG signal in the case that electrodes are positioned over the belly of a single muscle. These filter parameters were chosen by visual comparison across a range of values. While these preprocessing steps are in accordance with the literature and yield a signal with frequencies on a behavioral timescale though, as discussed in {+@sec:next_steps}, preprocessing of raw EMG signals is an area worth investigating the development and application of more advanced methods.

![Data from a single trial showing each step of preprocessing. The prototype preprocessing pipeline is highpass at 50Hz, rectification, and lowpass at 5Hz. The within-trial, per-channel means are subtracted from each trial. This matches what is typically done in the literature to find a correlate of intended force[@sangerBayesianFilteringMyoelectric2007;@churchlandNeuralPopulationDynamics2012a;@churchlandNeuralVariabilityPremotor2006;@sussillo2015]. There is significant room for improvement on this workflow, as discussed in the text.](images/data_analysis/fingers/preprocessing_steps.pdf){width=100% #fig:preprocessing_steps}



## Open-loop Finger Recordings

In our first preliminary experiment, a single subject produced flexions and extensions of each finger in the recording setup without any kind of artificial feedback. One trial was collected per finger movement in three blocks per session and one session per day over five days for a total of fifteen trials per finger movement. The purpose of this experiment is to determine the robustness over trials and sessions of EMG features for a simple low-contraction movement, as well as to determine the level of noise and artifacts in the data. Such baseline measurements are important to properly decompose variability due to electrode placement and exogenous noise from behavioral and physiological variability in order to ensure reproducibility of our results. Additionally, this baseline task may prove useful as a benchmark for later tasks in terms of testing analysis and decomposition techniques. A plot of all 32 channels for a single trial after preprocessing is shown in {+@fig:preprocessed_data}.

![All channels of data from a single trial after preprocessing. Note the difference in baseline for each channel. Ideally, each channel has a clear baseline of no activity, as further discussed in the main text.](images/data_analysis/fingers/preprocessed_data.pdf){width=100% #fig:preprocessed_data}

We first asked whether PCA applied to each individual trial would extract a single, high-variance component reflecting the dimensionality of the behavior. Since each finger movement is intuitively one dimensional, we predict that PCA would find a single high-variance component when run on each individual trial. As shown in {+@fig:PCA_variances}, this is generally the case, though there are some outlier trials. After inspecting these outlier trials, it is likely that the subject moved multiple fingers in these trials counter to the experimenter's instruction.

![Fraction of variance captured by the top 5 principle components for trials of single finger extensions and flexions. PCs were computed for individual trials. Trials were recorded from one subject for 10s each trial with finger movement frequency approximately 1Hz. Three blocks each with one trial per finger movement were recorded per day for 5 days for a total of 15 trials per finger. Electrodes were not removed between blocks but were removed between days. Each trial displays a single high-variance PC component.](images/data_analysis/fingers/PCA_variances.pdf){width=100% #fig:PCA_variances}

We next asked, since each trial appeared to be dominated by a single principle component, whether this top component was stable across trials and across days. If the top component is stable, it implies that the recording apparatus is robust to electrode movement between sessions. The top components for each movement after running PCA on each trial are shown in {+@fig:PCA_components}. While it is not typical to run PCA on individual trials, for the purpose of visually inspecting the PCA weights over EMG channels it is used here.

![The top principle component from each single finger movement trials plotted as weights across channels. PCA was computed for each trial individually. White horizontal lines show breaks between days when electrodes were removed and reapplied. Each trial's top component is relatively stable across trials and across days, though there is drift and dropout of weights. Measures to increase cross-session stability are discussed in the main text. Movements appear to vary between strong localization on single channels and broad activation across channels.](images/data_analysis/fingers/PCA_components.pdf){width=100% #fig:PCA_components}


The results here suggest that, at least up to linear decompositions, the features of low-contraction movements is relatively robust across sessions. As discussed in {+@sec:next_steps}, we will construct more reliable means to place electrodes on subjects' forearms to further increase repeatability. Another aspect of these results is our assumption subjects are producing the same contraction each time they move their finger, and at the same contraction level. That is, we assume they are recruiting the same motor units each movement. This may not be the case, and constructing new analyses which infer motor unit activations may be useful to mitigate this issue.

<!-- Electrode data from a single trial of a single session is held in a data matrix $X$ (n_electrodes, n_samples), and we wish to find a latent weight matrix $W$ (n_electrodes, n_components) which reconstructs $X$ by projecting latent trajectories $H$ (n_components, n_samples) into electrode space:

$$
X = W\cdot{H}
$$

$H$ is the activity of the latent processes, and $W$ is there mixing matrix. The columns of $W$ are the principal vectors spanning the latent subspace in electrode space. If we have new samples, we can project these new points onto this subspace:

$$
h_{new} = W^T\cdot{w_{new}}
$$

To justify this decomposition, we have to make some assumptions about the nature of the EMG signal, namely that the signal is linear instantaneous (each EMG sample can be instantly mapped to control space). The other assumption is that the basis $W$ should be orthonormal, that the columns of $W$ are orthogonal with unity norm. This ensures that the left inverse $W^{-1}$ is equal to the transpose $W^T$ such that:

\begin{align*}
X &= W\cdot{H} \\
W^{-1}\cdot{X} &= {H} \\
W^{T}\cdot{X} &= {H}
\end{align*}

See *Muceli 2014* for use of the Moore-Penrose pseudoinverse in place of the transpose when the columns of $W$ do not form an orthonormal basis. This would be the case for NMF. Is there a factorization that produces nonnegative, orthogonal coordinates? Or is the pseudoinverse okay? I will need to test this.

Stated in an information theoretic way, we want to minimize the reconstruction loss $\mathcal{L}$ for our derived encoder-decoder pair ($E$,$D$). We're decoding high dimensional activity into its latent dimensions, and encoding back into the high dimensional space. :

$$
\min_{E,D}{\mathcal{L}\left[X - EDX\right]}
$$

This way, forget about orthonormality and solve for an encoder and decoder directly. That is, $E\neq{D}$ is perfectly acceptable.

Each row of $D$ might be called a **spatial filter**, a linear combination of electrode activities into a surrogate, hopefully more intuitive space.

In general to find such a basis we must :

--> 

## Center-hold Reach-out Task

In this task, the 32-dimensional EMG electrode activity vector is mapped to a 2D force acting on a point mass shown on the screen. The mapping $M\in\mathbb{R}^{2x32}$ maps 8 "columns" each consisting of 4 electrodes placed in a line down the length of the forearm each to one of 2D root of unity. Each column of electrodes is thus mapped to one of 8 two-dimensional force vectors. In this experiment, the point mass has zero inertia and zero friction and as such displays a direct, though redundant, readout of the EMG signal. The task asks the subject to reach one of 32 equally spaced targets on each trial. Subjects must hold in the center of the task space for a designated period of time, after which the target appears. Subjects then have a time window reach the target. Data from one subject was recorded for three blocks in one session. Each block consisted of 32 trials, one per target in a randomized order.

The mapping between EMG and task space $M$ can be written as

$$
M = \begin{bmatrix}\tilde{M} & \tilde{M} & \tilde{M} & \tilde{M}\end{bmatrix}
$$

where $\tilde{M}$ consists of 8 equally spaced directions, one for each "column" of the 4 EMG electrode bands around the subject's arm:

$$
\tilde{M} =
\begin{bmatrix}
0  & 0.71  & 1   & 0.71   & 0  & -0.71  & -1  & -0.71 \\
1  & 0.71  & 0  & -0.71  & 1   & -0.71   & 0   & 0.71
\end{bmatrix}
$$

A graphic showing the mappings from electrodes to force directions is shown in {+@fig:columns}. While there are 8 possible force vectors the subject can modulate by controlling the electrode activity on each of her 8 columns, the EMG mapping is ultimately a projection onto the 2D plane. Since the EMG signal is nonnegative, the subject could technically modulate just four modes of electrode activity, the minimum number needed to span the task space, to reach all 32 targets. This mapping was chosen in order to provide a simple starting point to explore the virtual EMG task. There are no added environmental dynamics, only a redundant readout of force, and the signal is processed online in the same manner as in the offline analyses. This task or a variant we think can serve as a foundation upon which we can build more complex mappings and virtual environments. Task space trajectories from each block are shown in {+@fig:trajectories}. The fraction of trials resulting in hold timeouts, reach timeouts, and target hits are shown over blocks in {+@fig:hit_fraction}.

![Graphic showing the mapping between electrodes in the 32-channel center-hold reach-out experiment to eight 2D force directions in the virtual task space. Each of the eight columns consists of four electrodes each mapped to the same force direction (denoted with matching color) acting on a virtual point particle.](images/hardware/columns.pdf){width=70% #fig:columns}

![Point mass position trajectories in two-dimensional task space during the center-hold, reach-out task with 32 targets spaced evenly around the unit circle (shown with red borders). Color corresponds to target numbers, with target zero located at (0,1). Target order was randomized. Training was conducted over 3 blocks each with 32 trials, 1 trial per target.](images/data_analysis/center_hold/trajectories.pdf){width=60% #fig:trajectories}

![Fractions of trial outcome types for each block of the center-hold, reach-out task. Hit fraction increases on each trial, suggesting the beginnings of task learning. Hold timeout failure increases over trials as well, perhaps suggesting increased baseline excitation of muscles during the hold period. Part of learning to activate certain muscle modes is learning to inhibit others.](images/data_analysis/center_hold/hit_fraction.pdf){width=70% #fig:hit_fraction}

In this task, the subject's first goal is to interact through the mapping $M$ and learn the consequences of various motor activations. That is, they must internalize a model of the virtual environment, what might be called a system identification problem. Subjects must collect data containing motor outputs and their sensory consequences. To do this, they must explore the space of activity. We predict that over trials, subjects' EMG activities over channels will become more varied as they attempt new movement patterns to achieve their twin goals of learning to move in this new environment and reaching the target. Running PCA on each individual trial's EMG time series, we hypothesize that initially we will see multiple components share signal variance as subjects explore. Eventually, we would expect to see a decrease in this measure of movement complexity a subjects hone their reaching skill. Trial-level PCA component variance fractions are shown in {+@fig:PCA_trial_variance}. We find an initial decrease in the top component's variance fraction, though we expect many more trials will be needed to see skill acquisition in the form of a dominant movement mode per trial.

![Fraction of variance captured by the top five principle components when PCA is run on the EMG time series' of individual trials of the center-hold, reach-out task. Error bars are standard deviation. Over blocks, we see a slight decrease in the mean of the top component's variance fraction, though with high variance. This may suggest greater exploration within-trial, as less variance is captured by a single component over blocks, though it could reflect more varied dynamics across trials as the subject discovers new, task-relevant activations.](images/data_analysis/center_hold/PCA_trial_variance.pdf){width=70% #fig:PCA_trial_variance}

We might model this task as the subject selecting an EMG signal $x$ which minimizes the distance between a target position $b$ and the projection of the EMG signal through the mapping $M$ as well as minimizes the norm of $x$ in order to conserve metabolic energy. This optimization can be written as a regularized least squares problem:  

$$
\min_x\frac{1}{2}||Mx - b||^2_2 + \frac{\lambda}{2}||x||_2^2.
$$

This problem is known to have a unique minimum for $\lambda>0$ which is an approximation $Mx\approx b$ regardless of the shape or rank of $M$. This implies that the subject, if they are biophysically capable to do so, will learn distinct motor outputs for each target rather than reusing modes for multiple targets with different activation levels. That is the subject will, over time, learn to fractionate their muscle output to reach their goal in order to minimize effort. For instance, to reach the the target at position $(1,0)$ in Cartesian coordinates, the subject could activate a bespoke activity mode and activate a combination of two or more modes for targets at $\pm45^\circ$ from this central target. If this is the case, the model predicts that the dimensionality of the EMG signal will increase over the course of training as the subject learns to construct bespoke activity modes for each target.

In {+@fig:PCA_concat_variance} we compute PCA on the concatenation of all trials within blocks for which we predict an increase in the number of dominant EMG modes as subjects learn multiple movements to reach individual targets. We expect subjects to, over time, develop some number of bespoke movement modes to activate independently and as a composition to reach each target. We find a suggestion of this idea in the data with our basic PCA analysis. More trials, session, and subjects will be required to explore this idea, and we are investigating probabilistic measures of signal complexity such as entropy to formalize this hypothesis. One direction might be to further define this task a regularized regression with different regularization terms chosen normatively for different sections of the learning and control process, and fitting data to these predictions. For instance, early in training subjects may not optimize for target accuracy as much as for signal sparsity, whereas later in learning subjects may optimize for target accuracy and output signal magnitude.

![Fraction of variance captured by PCA computed on concatenated EMG time series concatenated over trials. Over blocks, we see variance shifting from the top component to other components. We hypothesize that across learning we would see the development of bespoke modes used in combination to reach individual targets. This would predict an increase in the complexity of the EMG time series over learning. Here we see suggestions of this prediction.](images/data_analysis/center_hold/PCA_concat_variance.pdf){width=70% #fig:PCA_concat_variance}

<!-- 1. System Identification -- learning a transition function $p(y_t|x_t, u_t)$
    - How do you learn the unknown observation model from data?

1. Policy Optimization
    - Once dynamics are learned (or at least stable?), how do we form a policy that is generalizable to new tasks under these dynamics?
    - This is the control problem.

It's safe to assume that these processes are happening in parallel. Because we have complete and arbitrary control over the observation mapping, we can ask the subject to interact through a  dynamic that is intuitive (informative prior) or unintuitive (uninformative or inhibitive prior). Each scenario, we hypothesize, will elicit different strategies for learning and control.

-->


\subsection{Control}\label{control}

Is LQR (as it's claimed to be) a reasonable model for feedback control
and error reduction + variability prediction for dimensionality
reduction-based motor interface

(task reads out from D muscles, find modes of that data; do PCA to get K
\textless{} D dimensions, controller only responds to motion in those K
directions)---does behavior + motor activity follow LQR? this question
has already been asked, but it hasn't been asked for this kind of
high-to-low dim mapping. It's been asked in tasks where muscles haven't
been directly in control (Bolero 2009). Todorov: do a task, look at
muscle signal. Muscles that aren't necessary for task have higher
variability b/c they're not being optimized for task (but does't
introduce perturbations). Also see Loeb (2012) for a negative result
saying that muscle coordination is habitual rather than optimal, but it
has issues (low \# muscles). Can we replicate previous reaching
optimality results in our set-up? What's unique about our set-up is the
PCA/dimensionality reduction in muscle activity space. This is important
because you can create arbitrary muscle-cursor mappings, so you have to
learn a new skill/mapping. This is different than perturbing a
fundamental movement and forcing adaptation, which is what has been
previously done. For our task, the participants actually have to learn a
new task/mapping, rather than just do what they already know and be
robust to perturbations. We test the LQR hypothesis once they've learned
the task, because LQR isn't a learning theory, it's a theory about
optimal control. We can see if, once people learn a new skill, their
behavior is optimal wrt LQR theory. If we establish this, then we can
think about how this LQR model is actually learned (enter RL).

\begin{quote}
Our results are consistent with a recently described model in which an
optimal feedback control policy is calculated independently for each
potential target and a weighted average of these policies (that is,
feedback gains) is computed at each point in time based on the relative
desirability of each target50. Notably, this model, which pre-dicts
averaging of feedback gains, can also account for spatial (that is,
trajectory) averaging in go-before-you-know tasks. We submit that our
result showing feedback gain averaging, coupled with previous work
demonstrating trajectory averaging, provides strong support for the
compelling idea that the CNS, under cases of target uncertainty, encodes
in parallel multiple motor plans, along with their associated control
policies, for competing action options. (Wolpert Nature 2018 competing
control policies)
\end{quote}

Nashed 2014 -- short-latency R1 and long-latency R2 responses (60ms;
45--75ms; 75--105 ms) stretch responses R1 show dexterity (Andrew Pru
2019,2020) in holding and in reaching

{[}@takeiTransientDeactivationDorsal2021{]} cooled areas of motor cortex
and mapped them onto OFC-- limitation of this is the behavior, it's a
postural maintenance task, not a reach or a complex movement\ldots OFC
is definitely the best model here. We should be mindful about modeling
things that go beyond this model, they should in some way be able to
reduce down to OFC in the limit of postural maintenance, perhaps\ldots{}

\subsubsection{Adaptation of Reaching}\label{adaptation-of-reaching}

Sinusoidal over trials visuomotor rotations allowed experimenters to
disentangle perturbation error sequence over trials in ``driven'' and
``undriven'' dimensions by knowing the frequencies of the rotational
perturbation. Explicit strategies were given by participants before
moving, all errors based off this
{[}@miyamotoImplicitAdaptationCompensates2020{]}. This is a very clever
experimental design but leaves many questions. Particularly, implicit
adaptation seems to be driven by a combination of performance error and
sensory prediction error, but how can we tease this apart?

\begin{quote}
Although little is known about the specific error signals that drive
these different processes, an intriguing possibility is that distinct
components of implicit learning are driven by performance errors and
sensory-prediction errors.
\end{quote}

\begin{quote}
The convergent findings of the SEM and time lag analyses, based on the
amplitudes and temporal structure of implicit and strategic adaptive
responses, point to an implicit learning process that actively responds
to compensate low-fidelity explicit strategy.
\end{quote}

\begin{quote}
Our simulation also reproduced the results of the SEM analysis and the
temporal lag analysis, which demonstrates that the low-noise process
(which models implicit learning) lags behind and effectively compensates
for the inappropriate behavior of the high-noise process (which models
strategy), a result that can be predicted by mathematical derivation
(Supplementary Math Note).
\end{quote}

In our experiment, we might call all learning implicit, since there is
so much unknown what would we constitute as a strategy? Think we need to
make clear the difference between learning and adaptation. Adaption, as
used here, is correction to a perturbation over trials of a
well-practiced movement. Learning is the discovery of unfamiliar
movement patterns through trial and error.

\begin{quote}
An important idea in motor skill learning research is that motor
learning proceeds from predominantly explicit to implicit states as a
learner develops from novice to expert.
\end{quote}

This is interesting, as in our experiment we might think the opposite?
Depends on how we define explicit and implicit. Here the definition is
loosely deliberate and effortful versus automatic and intuitive. Their
point here might be mirrored in overall muscle activation, early on it's
actively searching. But what for? And how? We might think of this as a
foraging task, searching for information about the unknown mapping--
movements as hypotheses.

\begin{itemize}
\tightlist
\item
  prisms
\item
  rotations
\item
  forcefield
\item
  nothing -- van Beers variability
\end{itemize}

\begin{quote}
The vast majority of research in motor learning studies this capacity
through adaptation para- digms in which a systematic perturbation is
introduced to disrupt a well-practiced behavior, such as point-to-point
reaching. {[}@adrianTheoreticalModelsMotor2012{]}
\end{quote}

\begin{itemize}
\tightlist
\item
  classic reaching adaptation --\textgreater{} this is a different goal

  \begin{itemize}
  \tightlist
  \item
    shadmehr
  \item
    krakauer
  \end{itemize}
\item
  unperturbed movements

  \begin{itemize}
  \tightlist
  \item
    van beers
  \end{itemize}
\end{itemize}

{[}@Krakauer2019;@Shadmehr2008{]}

There exist a handful of prior studies mapping EMG activity and finger
joint angles directly to virtual stimuli, though few are focused on the
learning process and none have the input dimensionality we aim to
achieve in work proposed here.

{[}@manleyWhenMoneyNot2014 @vanbeersMotorLearningOptimally2009
@vanbeersRandomWalkMotor2013{]}

\subsubsection{Arbitrary Visuomotor
Mappings}\label{arbitrary-visuomotor-mappings}

{[}@Mussa-IvaldiSensoryMotorRemapping2011{]}

\begin{quote}
The null space generated by our glove-cursor map had effectively 17
dimensions (19 􏰇 2). We observed a marked tendency of subjects to reduce
the amount of motion in this null space (Fig. 4). The selective
reduction of null-space motion is particularly important because it may
reveal how the Euclidean metric of the task space (the monitor) is
effectively ``imported'' into the coordination of hand. The tendency to
generate finger motions with smaller null-space components suggests that
the movements tend to remain confined to subspaces that are minimum-norm
images of the cursor space. This observation provides us with further
evidence that the motor system is effectively capturing the metric
structure of the controlled space and that it uses this metric as a
basis to form coordinated motions of the fingers.
{[}@MosierRemappingHandMovements2005{]}
\end{quote}

\begin{quote}
\ldots we have observed a general tendency of subjects to reduce the
amount of finger motion, again suggesting that they are learning
trajectories, not just final positions or by points. Our data also show
a strong and progressive decrease of movement variability from day to
day along the entire motion. This is in sharp contrast with the
hypothesis that, through practice, subjects learn to export increasing
amounts of variability into the null space to achieve a less-variable
task execution.
\end{quote}

\begin{quote}
hand configurations and glove signals are related by a nonlinear
isomorphism, rectilinear motions of the cursor on the monitor are not
compatible with rectilinear motions in the space of finger-joint
coordinates.
\end{quote}

what is the natural space of motor activations? what do these spaces
look like? how are they mapped?

There are several studies using non-EMG-driven sensorimotor mappings to
study human motor control and learning.

\begin{itemize}
\tightlist
\item
  Remapping Hand Movements in a Novel Geometrical Environment
  https://www.ncbi.nlm.nih.gov/pubmed/16148276
\end{itemize}

vocoder machine bell labs

Hinton, Fells

palsy study

takehome: humans are really good at learning tasks like these,
especially with their hands. this type of dexterity is specific to
primates if not humans. let's use this ability to understand and try to
model how this learning process unfolds.

\textbf{\emph{What does this give us that a force-field reaching task
can't?}}

{[}@nazarpourFlexibleCorticalControl2012{]}

\subsubsection{Skill Learning Tasks}\label{skill-learning-tasks}

\begin{itemize}
\tightlist
\item
  skill learning tasks

  \begin{itemize}
  \tightlist
  \item
    ball and cup
  \item
    dart throwing tasks
  \end{itemize}
\end{itemize}

\subsubsection{Learning in Cortical
Interfaces}\label{learning-in-cortical-interfaces}

\begin{itemize}
\item
  cortical BMI work

  \begin{itemize}
  \tightlist
  \item
    Batista papers, lee miller papers
  \end{itemize}
\item
  speech learning -- analogy to speech
\item
  bird vocal learning
\item
  we're doing the same experiment, at the muscle level
\item
  try to convince why this is useful, but not too hard
\end{itemize}

\subsubsection{Skill Learning in Myolectric
Interfaces}\label{skill-learning-in-myolectric-interfaces}

``Motor learning explored with myoelectric and neural interfaces''
(Nazapour, Jackson)

\begin{quote}
Experimental myoelectric and neural interfaces can create simplifi ed
sensorimotor worlds in which the map from motor commands to effectors
can be precisely controlled. These abstract sensorimotor paradigms offer
an opportunity to explore further fundamental motor learning questions
that would otherwise be obscured by the anatomical and biomechanical
complexity of the limbs. By introducing highly artifi cial and unusual
sensorimotor mappings, we may ask whether the human motor system is
constrained to naturalistic behaviors or can adapt to circumstances
outside the normal ethological repertoire
\end{quote}

\begin{quote}
biomimetic interfaces v. abstract interfaces
\end{quote}

\begin{quote}
flexible use of divergent pathways to form new, task-specific muscle
synergies
\end{quote}

\begin{quote}
There is an approximately linear relationship between the rectifi ed EMG
and force under conditions of isometric muscle contraction.
\end{quote}

\begin{quote}
several hundred trials of practice
\end{quote}

\begin{quote}
the distal muscles are not limited to a small number of fixed synergies
\end{quote}

\begin{quote}
Optimal Feedback Control is one of our most complete descriptions of the
computations that must be performed by the motor system. But it says
little about how minimum intervention policies are learned and
implemented by neural elements. It remains to be seen whether its
pre-dictions can be reconciled with the neuroanatomical evidence for
divergent pathways underlying muscle synergies.
\end{quote}

\begin{quote}
We find that with training, comparable results can be obtained
irrespective of which distal muscles are used to control the task.
\end{quote}

\begin{quote}
task-specific correlation structure
\end{quote}

\begin{quote}
{[}CM connections{]} bypass synergies hard-wired into subcortical and
spinal circuitry, allowing the motor cortex considerable fine control
over specific muscles. The distal bias of cortico-motoneuronal
connections likely explains why these muscles are better suited to
control of abstract myoelectric interfaces than proximal muscles.
\end{quote}

\begin{quote}
A typical cell may exhibit post-spike effects in several forearm and
hand muscles, which defi nes the muscle fi eld for that cell (Fetz \&
Cheney, 1978; Jackson et al., 2003). Across the population a wide
variety of different muscle fi elds are seen for individual
corticospinal neurons.
\end{quote}

\begin{quote}
We have already seen how redundancy in the mapping from muscles to
movement can be exploited by distributing effort optimally across
multiple muscles. We now fi nd that redun-dancy in the mapping from
cortical neurons with divergent muscle fi elds provides a further
opportunity to distribute effort among multiple muscle synergies. The
picture that emerges is one of hierarchical levels of redundancy within
the motor system (Figure 4.6). At each level, convergence onto fewer
dimensions in the level below means a multitude of redundant activity
patterns are consistent with the desired goal. At the same time,
divergence in the descending pathways allows behavior to be optimized
for high-level task goals like accuracy. However, a high-dimensional
control space is a double-edged sword; the fl exibility to optimize the
motor pattern for any abstract high-level goals comes at the cost of a
large search space within which this pattern must be sought. The
learning mechanisms that allow the motor system to achieve optimality
are the subject of much current research.
\end{quote}

\begin{quote}
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Ctrl-labs motor BMI {[}@mendezguerraNoninvasiveRealtimeAccess2021{]} --
decoding motor units and tendon electric fields at the wrist, but
nothing really about learning\ldots{}

\begin{quote}
Due to the unstructured nature of the exploration period, we first
decomposed motor unit firing rates into separate components via
non-negative matrix factorization (NMF) to identify groups of units that
were often mutually active. We fixed this number of components to 3,
aligning with the instructions given to the participant to ultimately
select 3 representative motor units.
{[}@formentoNoninvasiveBrainmachineInterface2021{]}
\end{quote}

NMF is X \approx WH

\begin{quote}
Since the relative scales of the projections (W) and its components (H)
are typically arbitrary, we resolved ambiguity by scaling each component
to unit L2-norm and scaling its corresponding transformation by the
appropriate reciprocal factor.
\end{quote}

\begin{quote}
We trained 8 participants over 6 consecutive days using this system on a
center-out task requiring both individual and simultaneous control of
three motor units. We showed that participants demonstrated improvements
in performance both within and across days. Through comparisons to
isometric, ramp-and-hold contractions, we provide evidence that
neurofeedback enabled participants to expand their ability to control
individual motor units outside of naturalistic movement constraints.
\end{quote}

\begin{quote}
Taken together, these results reveal the center-out task enabled both a
significant, population-level increase in dimensionality relative to
during stereotyped, isometric contractions and an increase in
unexplained variability in the unselected motor unit population.
{[}@formentoNoninvasiveBrainmachineInterface2021{]}
\end{quote}

Dimensionality here is the ``participation ratio'':

\begin{quote}
The participation ratio (PR) was computed to quantify the dimensionality
of the iEMG and firing rate data83--85. The PR is a metric computed on
the covariance matrix of a feature and represents the approximate
dimensionality of the manifold spanned by that feature; a higher
participation ratio means more principal components are needed to
explain a given proportion of the feature's variance.
\end{quote}

``performance levels and rates of improvement were significantly higher
for intrinsic hand muscles relative to muscles of the forearm.''
{[}@Dyson2018{]}

\subsubsection{synergies and learning}\label{synergies-and-learning}

\begin{quote}
the motor system can acquire new muscle synergies during motor skill 575
learning, especially for skills that cannot be adequately accomplished
by deploying preexisting 576 synergies\ldots{} The exact mechanism
responsible for 581 this learning has not been established. But during
early skill learning, the motor system may 582 discover the direction of
synergy change by exploiting and modulating the intrinsic variability
583 of the synergies and of their temporal activations, and subsequently
drive this change by 584 reinforcing the synergy patterns that lead to
reward-producing actions (Cheung et al., 2020b) in 585 a manner
analogous to how reinforcement learning relies on action exploration
{[}@cheungApproachesRevealingNeural2021{]}
\end{quote}

\subsubsection{Berger et al.~2013}\label{berger-et-al.-2013}

{[}@BergerDifferencesInAdaptationRates2013a{]}

Berger et al.'s preprocessing - lowpassed butterworth at 5Hz -
normalized to MVC calibration - periodic baseline noise substraction -
choosing synergies by uniformity in force direction (pretty arbitrary)

Using EMG in a learning experiment is not unheard of. Berger et al.~2013
use EMG with 13 muscles to test whether learning new synergy
combinations for a task is more difficult that recombining existing
synergies@Berger2013a. As we would expect, learning new synergy
combinations is more difficult. I would argue that the demand in their
``incompatible virtual surgeries'' is too strict, that we need to more
carefully design synergy perturbations to develop a model of learning in
such a task.

Berger et al.~fit a muscle-space to force-space mapping \(H\) using a
force-driven calibration task, and a synergy-space to muscle-space
mapping \(W\) using NMF.

\begin{align*}
    f &= Hm \\
    m &= Wc
\end{align*}

\(\dim(m)=M\) muscles, \(\dim(c)=N\) synergies, and \(\dim(f)=D\)
dimensions of task space where \(M>N>D\). Because \(H\) and \(W\) are
rectangular, they have at most rank \(D\) and \(N\), and we constrain
these matrices to be full rank. There are three key subspaces: the
nullspace of \(H\) mapping muscle activations to 0, the column space or
range of \(W\) mapping synergy activations to muscle activations, and
the common subspace between these two. That is, there are synergy
activations which generate muscle activations which lie in the null
space of \(H\). The paper uses this fact to develop mappings that
specifically rotate muscle activations produced by synergies into the
null space of \(H\) which were not there prior to rotation. The
dimensionalities of these subspaces are defined:

\begin{align*}
\dim(\mathrm{null}(H)) &= M - D && \text{muscle vectors $\rightarrow$ 0} \\
\dim(\mathrm{col}(W)) &= N && \text{synergy activations $\rightarrow$ muscle subspace}\\
\dim(\mathrm{null}(H)\cap\mathrm{col}(W)) &= N - D && \text{synergy activations $\rightarrow$ 0} \\
\end{align*}

In the paper, the authors find an orthonormal basis \(W_o\) for the
range (column space) of the synergy weight matrix \(W\) (presumably
using a QR factorization) and find the nullspace \(H_{null}\) of \(H\).
These computations are done presumably through QR factorizations (an
orthonormal basis multiplied by a rotation and scaling) by finding \(Q\)
in the first case and finding the latter \(M-D\) columns \(Q_2\) of
\(Q = [Q_1 \, Q_2]\) which are \(H_{null}\) in the second case:

\begin{align*}
    W &= Q_W^{M\times M}R_W^{M\times N} \\
      &= \left[Q_{W,1}^{M\times N}\,Q_{W,2}^{M\times M-N}\right]\begin{bmatrix}R_{W,1} \\ 0 \end{bmatrix} \\
    W_o &= Q_{W,1}^{M\times N} \\
    W_o^T &= Q_{W,1}^{T, N\times M} \\
    H^T_{M\times D} &= Q_H^{M\times M}R_H = \left[Q_{H,1}^{M\times D}\,Q_{H,2}^{M\times M-D}\right]\begin{bmatrix}R_{H,1} \\ 0 \end{bmatrix} \\
    H^T_{null} &= Q_{H,2}^{M\times M-D}
\end{align*}

To find each of the three subspace, we take the SVD of the composition
\(W^TH^T\)

\begin{align*}
    W_o^TH^T_{null} &= Q_{W,1}^{T, N\times M} Q_{H,2}^{M\times M-D} \\
    &= Q^T_WQ_H && \dim(N \times M-D) \\
    &= U_{N\times N}\Sigma_{N \times M-D} V^T_{M-D\times M-D} \\
    H^T_{null}V &= W_oU\Sigma
\end{align*}

Now we can pick out the three subspaces using the SVD

\begin{align*}
    W_c &= W_oU[1:N-D] && \text{synergy activations $\rightarrow$ muscle activations in task null space}\\
    H_c &= H_{null}[1:N-D] && \text{synergetic muscle activations $\rightarrow$ 0} \\
    W_{nc} &= W_oU[N-D+1:N] && \text{synergy activations $\rightarrow$ nonzero muscle activations}\\
    H_{nc} &= H_{null}V[N-D+1:M-D && \text{non-synergetic muscle activations $\rightarrow$ 0} \\
\end{align*}

To construct new mappings, the authors construct rotations to alter
muscle activation vectors by rotating them from \(W_nc\) and remaining
in \(W_nc\) and from \(W_nc\) into \(H_nc\). In the first case this
alters the mapping by changing the effective muscle activations produced
by the existing (learned) synergetic actions. That is, muscle
activations putatively produced by synergetic action will be altered to
produce different forces in task space (compatible rotations). In the
second case, muscle activations putatively produced by existing
synergetic action (via W) will be mapped into the null space of \(H\)
and produce zero force in task space (incompatible rotations).

A key critique of this paper is that such a transformation is too harsh.
The compatible rotation allows you to recombine the same muscle
patterns, the incompatible doesn't allow you to use existing
coactivation patterns at all. The authors do see new synergies emerging
even after their training session, consisting of:

\begin{itemize}
\tightlist
\item
  16 trials of maximum voluntary contraction in 8 directions
  (calibration)
\item
  72 trials using force control (calibration)
\item
  24 trials familiarization
\item
  144 trials baseline
\item
  288 trials surgery
\item
  144 trials washout
\item
  144 trials baseline
\end{itemize}

After 288 trials subjects aren't able to complete the task for some
movement directions.

\subsubsection{Nazarpour 2012 J.Neuro}\label{nazarpour-2012-j.neuro}

{[}@nazarpourFlexibleCorticalControl2012{]}

x Flexible Cortical Control of Task-Specific Muscle Synergies
https://www.jneurosci.org/content/32/36/12349

Fig. 4A -- cursor controlled muscles begin to dissociate from non cursor
controlled muscles.

Feedforward processing to muscle fields / tunings in the presence of
signal dependent noise

Feedback processing based on visual errors

\subsubsection{Radhakrishnan 2008}\label{radhakrishnan-2008}

{[}@radhakrishnanLearningNovelMyoelectricControlled2008{]}

x Learning a Novel Myoelectric-Controlled Interface Task ---
Radhakrishnan, 2008
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2576223/

proprioception is not required to learn nonintuitive MCI mappings

several hundred trials subjects learned pointing with six muscles

prism adaption requires active movement; efference copy implicated if
proprioception doesn't seem to be required

control models Fig 10

\subsubsection{de Rugy 2012 - Habitual not
Optimal}\label{de-rugy-2012---habitual-not-optimal}

{[}@derugyMuscleCoordinationHabitual2012{]}

just because it's harder to adapt to incompatible surgeries doesn't mean
that there are fixed synergies, it just means there are multiple
timescales of adaptation available in the neural control hierarchy --
diversity in the neural controller depending on context

learning inverse model may be separate from learning to optimize
trajectories on top of that model -- some type of ``fine tuning''

skill acquisition (slow, constructing novel synergies) vs.~motor
adaptation (less slow, adapting existing synergy activations)

It's a good test, but it pushes the optimal control framework too hard?
perhaps we need a model for what ``good enough'' is? If we penalize
moving to a new controller from a previously optimized movement, the
findings make sense. An optimal control model would predict the exactly
optimal coordination patterns for the new scenario, it wouldn't say
anything about adaptation from an old solution to a new one. This is why
we need to develop a model of adaptation that formalizes this scenario
not of kinematic perturbations (noise during movement), but to a drastic
change in the plant itself (e.g.~muscle failure).

\subsubsection{Mussa-Ivaldi 2019}\label{mussa-ivaldi-2019}

\begin{quote}
Earlier theoretical work by Jordan and Rumelhart {[}14{]} considered how
the learning of actions can be viewed as the concurrent learning of for-
ward and inverse models of actions. \textbf{They introduced the concept
of distal learning, where the learner has to find a mapping from desired
outcomes to actions in order to achieve a desired outcome. To do so, the
subject begins by forming a predictive forward model of the
transformation from actions to distal outcomes. Such transformations are
often not known a priori, thus the forward model must generally be
learned by exploring the outcomes associated with particular choices of
action. Once the forward model has been at least partially learned, it
can be used to guide the learning of an inverse model that predicts the
action needed to achieve the distal outcome.} Mussa-Ivaldi2019
\end{quote}

\begin{quote}
Our findings are consistent with the hypothesis that learning proceeds
through the concurrent evolution of cou- pled forward and inverse models
of the body-to-object mapping established by the BoMI. Mussa-Ivaldi2019
\end{quote}

\begin{quote}
Not being square, the matrix H does not have a unique inverse. But there
exist infinite ``right inverses'' that combined with H yield the K x K
identity matrix in the task space of exter- nal control signals. Each
such right inverse transforms a desired position of the controlled
object into one particular set of values for the body signals. We
consider users to be competent when they are able to move their body
successfully in response to a presented target for the controlled
object. Mathematically, we consider this as finding one right inverse G
of the map- ping H, out of a multitude of possible and equally valid
choices. Mussa-Ivaldi2019
\end{quote}

Gradient learning of a forward and inverse model (mapping):

\begin{align*}
    \hat{H}_{n+1} &= \hat{H}_n + \epsilon(p_n - H_nq_n)q_n^T  \\
    G_{n+1} &= G_n - \eta\hat{H}_n^Te_nu_n^T \\
    e_n &= p_n - u_n
\end{align*}

\begin{quote}
The comparison between model predictions and actual data in Fig 3
indicates that our proposed model of learning is sufficient to explain
the data. However, the mechanism we propose is not necessary; we cannot
rule out other possibilities, such as reinforcement learning.
{[}\ldots{]} This agreement between model and experimental results does
not exclude the possibility of alternative learning mechanisms, such as
a direct learning of the inverse model {[}24{]} or the use of
reinforcement learning {[}25{]} to acquire an action policy that would
play the role of the inverse model. Mussa-Ivaldi2019
\end{quote}

How do we break a simple gradient model? On a task that is more
difficult? will learning take longer? - savings phenomenon -

\begin{quote}
Although the interface forward map is linear (Methods, Eq (5)), this is
a many-to-one map admitting a multitude of inverses. This ``redundancy''
opens the possibility of successful linear and nonlinear inverse maps.
Redundancy also leads to an important consideration about gradient
descent learning. The reaching error surface in the space of the inverse
model elements does not have a unique minimum, but a continuously
connected set of minima corresponding to the null space of the forward
map. In the metaphor of a skier descending from a mountain following the
gradient, this space of equivalent inverse models corresponds to a flat
elongated valley at the bot- tom of the mountain. Anywhere along the
valley is a valid end to the ride, as it corresponds to a valid inverse
model. The inverse model on which the steepest descent ends depends on
the initial conditions, as predicted by the dynamical model (see Fig
3b--evolution of the norm of the inverse model error), as well as on the
realization of the noise employed in any given simulation of the
learning model.
\end{quote}

\begin{quote}
Although the two-dimensional subspace formed by the first two PCs
captured a large fraction of the total variance of body motions, it did
not necessarily reflect the natural up-down/left-right orientation of
the display monitor. Therefore, following calibration and PC extraction,
there was a customization phase in which users were allowed to set the
origin, orientation, and scaling of the coordinates in task space, based
on their preference.
\end{quote}

Subjects have prior knowledge of their directions in task space?

x 90\% isn't enough -- Follow-up on the previous paper -- critiques
``direct evidence'' https://www.biorxiv.org/content/10.1101/634758v1

\begin{itemize}
\tightlist
\item
  Structured variability of muscle activations supports the minimal
  intervention principle of motor control
  https://www.ncbi.nlm.nih.gov/pubmed/19369362
\end{itemize}

\include{end_of_chapter}
\end{document}